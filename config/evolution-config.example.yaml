# EmbedEval Evolution Configuration
# Copy this file to evolution-config.yaml and customize for your needs

# =============================================================================
# YOUR MODELS - Add all embedding models you want to test
# =============================================================================
models:
  # Local models (Ollama) - FREE
  ollama:
    enabled: true
    models:
      - nomic-embed-text        # Good general purpose, 768 dims
      - mxbai-embed-large       # Larger, more accurate
      - all-minilm              # Smaller, faster
      - snowflake-arctic-embed  # Good for code
      # Add your custom Ollama models:
      # - my-finetuned-model
    
  # OpenAI - PAID
  openai:
    enabled: false  # Set to true if you have OPENAI_API_KEY
    models:
      - text-embedding-3-small  # $0.02/1M tokens, 1536 dims
      - text-embedding-3-large  # $0.13/1M tokens, 3072 dims
      # - text-embedding-ada-002  # Legacy
    
  # Google Gemini - PAID (generous free tier)
  gemini:
    enabled: false  # Set to true if you have GEMINI_API_KEY
    models:
      - text-embedding-004      # Latest, best quality
      # - embedding-001         # Older
    
  # Cohere - PAID
  cohere:
    enabled: false  # Set to true if you have COHERE_API_KEY
    models:
      - embed-english-v3.0
      - embed-multilingual-v3.0  # Good for non-English
    
  # Voyage AI - PAID
  voyage:
    enabled: false  # Set to true if you have VOYAGE_API_KEY
    models:
      - voyage-2
      - voyage-large-2
      - voyage-code-2  # Optimized for code
    
  # HuggingFace - FREE (local) or PAID (API)
  huggingface:
    enabled: false
    models:
      - BAAI/bge-small-en-v1.5
      - BAAI/bge-large-en-v1.5
      - sentence-transformers/all-MiniLM-L6-v2

# =============================================================================
# ENHANCEMENT LAYERS - What techniques to try
# =============================================================================
enhancements:
  # Query processing before embedding
  query:
    - none              # Raw query (baseline)
    - lowercase         # Simple normalization
    - expand_synonyms   # Add synonym terms
    - expand_llm        # LLM expands query
    - hyde              # Hypothetical Document Embeddings
    - hyde_multi        # Multiple HyDE generations
    - step_back         # Step-back abstraction
    - multi_query       # Generate query variants
    
  # Retrieval method
  retrieval:
    - cosine            # Standard cosine similarity
    - hybrid_rrf        # Reciprocal Rank Fusion (dense + BM25)
    - hybrid_linear     # Linear combination
    # - late_interaction  # ColBERT-style (slower)
    
  # Reranking (the big performance booster!)
  reranking:
    - none              # No reranking (baseline)
    - bm25_rerank       # BM25 score reranking (free)
    - mmr               # Maximal Marginal Relevance (free)
    - cross_encoder_small  # ms-marco-MiniLM (free, local)
    - cross_encoder_large  # BGE-reranker (free, local)
    # Paid reranking options:
    # - llm_pointwise   # LLM scores each ($$)
    # - llm_listwise    # LLM ranks list ($$$)
    # - cohere_rerank   # Cohere API ($)
    # - jina_rerank     # Jina API ($)
    
  # Document chunking strategy  
  chunking:
    - none              # Full documents
    - fixed_512         # 512 token chunks
    - semantic          # Semantic boundaries
    - parent_child      # Small chunks, return parent
    - sentence          # Sentence-level
    
  # Multi-stage retrieval
  multi_stage:
    - none              # Single stage
    - two_stage         # Retrieve k1 → rerank → k2
    - three_stage       # k1 → filter → k2 → verify

# =============================================================================
# CONSTRAINTS - Your requirements
# =============================================================================
constraints:
  # Cost constraints
  max_cost_per_query: 0.001    # Max $ per query (0 = free only)
  
  # Latency constraints  
  max_latency_ms: 500          # Max milliseconds per query
  
  # Only use local/free models?
  local_only: true             # Set false to include paid APIs
  
  # Minimum quality threshold
  min_accuracy: 0.6            # Don't consider configs below this

# =============================================================================
# EVOLUTION SETTINGS
# =============================================================================
evolution:
  # How many generations to evolve
  generations: 10
  
  # Population size per generation
  population_size: 20
  
  # How many top configs to keep
  elite_count: 3
  
  # Mutation probability
  mutation_rate: 0.25
  
  # Try all combinations first before evolving?
  exhaustive_first: true       # Recommended for small search space
  
  # Max combinations to try (if exhaustive)
  max_combinations: 500

# =============================================================================
# YOUR DATA
# =============================================================================
data:
  # Path to your queries (JSONL format)
  queries: ./examples/eval-queries.jsonl
  
  # Path to your document corpus (JSONL format)  
  corpus: ./examples/eval-corpus.jsonl
  
  # Or use built-in benchmark datasets
  # dataset: mteb-nfcorpus
  # dataset: squad
  # dataset: hotpotqa

# =============================================================================
# OUTPUT
# =============================================================================
output:
  # Where to save results
  dir: ./results
  
  # Generate HTML report
  html_report: true
  
  # Save all tested configurations
  save_all_configs: true
  
  # Recommendation format
  recommendations:
    top_k: 5              # How many top configs to recommend
    show_tradeoffs: true  # Show cost/speed/quality tradeoffs
    explain_why: true     # Explain why each config is good

# EmbedEval Strategy System - Implementation Summary

## âœ… What Was Built

### 1. Composable Strategy System

**Core Architecture:**
```
Strategy = Array of Stages
Stage = { type, name, config, execute() }
```

**Files Created:**
- `src/strategies/types.ts` - Core types (StrategyContext, StageConfig, etc.)
- `src/strategies/registry.ts` - Strategy registry and executor
- `src/strategies/chunking/index.ts` - Chunking implementations
- `src/strategies/retrieval/bm25.ts` - BM25 retrieval
- `src/strategies/fusion/index.ts` - Fusion methods (RRF, Weighted)
- `src/strategies/reranking/index.ts` - Re-ranking (LLM, MMR)

### 2. Available Strategies

**Chunking Strategies:**
- `fixed-size` - Fixed size chunks with overlap
- `semantic` - Paragraph-based semantic chunks
- `sliding-window` - Sliding windows with configurable step

**Retrieval Methods:**
- `bm25` - Classic Okapi BM25
- `embedding` - Cosine similarity (existing)

**Fusion Methods:**
- `rrf` - Reciprocal Rank Fusion
- `weighted` - Weighted score combination

**Re-ranking:**
- `llm` - LLM-based relevance scoring
- `mmr` - Maximal Marginal Relevance for diversity

### 3. Predefined Strategy Pipelines

```typescript
PREDEFINED_STRATEGIES = {
  'baseline': [embed â†’ retrieve]
  'fixed-chunks': [chunk(fixed) â†’ embed â†’ retrieve]
  'semantic-chunks': [chunk(semantic) â†’ embed â†’ retrieve]
  'hybrid-bm25': [bm25 â†’ embed â†’ retrieve â†’ rrf-fusion]
  'llm-reranked': [embed â†’ retrieve â†’ llm-rerank]
  'mmr-diversity': [embed â†’ retrieve â†’ mmr-rerank]
  'full-pipeline': [chunk â†’ bm25 â†’ embed â†’ retrieve â†’ rrf â†’ llm-rerank]
}
```

### 4. CLI Commands

**List Strategies:**
```bash
embedeval strategy --list
```

**Test Strategy:**
```bash
embedeval strategy --test hybrid-bm25
```

**Run A/B Test with Strategies:**
```bash
# Compare different strategies on same model
embedeval ab-test \
  --variants ollama:nomic-embed-text \
  --strategies baseline,fixed-chunks,semantic-chunks,hybrid-bm25 \
  --dataset ./data/queries.jsonl \
  --corpus ./data/corpus.jsonl

# Compare different models with same strategy
embedeval ab-test \
  --variants ollama:nomic-embed-text,openai:text-embedding-3-small \
  --strategies hybrid-bm25 \
  --dataset ./data/queries.jsonl

# Full factorial: all models Ã— all strategies
embedeval ab-test \
  --variants ollama:nomic-embed-text,openai:text-embedding-3-small \
  --strategies baseline,hybrid-bm25,llm-reranked,mmr-diversity \
  --dataset ./data/queries.jsonl
```

### 5. Configuration Support

**YAML Config with Strategies:**
```yaml
variants:
  - id: variant-1
    name: "Semantic Chunks + BM25"
    strategy: semantic-chunks
    provider:
      type: openai
      model: text-embedding-3-small
  
  - id: variant-2
    name: "Hybrid with LLM Rerank"
    strategy: full-pipeline
    provider:
      type: openai
      model: text-embedding-3-large
```

## ðŸ§ª Testing

**Unit Tests:**
- `tests/strategies.test.ts` - Tests for all strategy components

**Integration Test:**
- `tests/integration-test.ts` - Full pipeline validation

**Run Tests:**
```bash
# Unit tests
npm test

# Integration test
npx ts-node tests/integration-test.ts

# Manual CLI test
npm run dev -- strategy --list
npm run dev -- strategy --test hybrid-bm25
```

## ðŸš€ Usage Examples

### Example 1: Chunking Strategy Comparison
```bash
embedeval ab-test \
  --name "Chunking Comparison" \
  --variants ollama:nomic-embed-text \
  --strategies baseline,fixed-chunks,semantic-chunks,sliding-window \
  --dataset ./examples/sample-queries.jsonl \
  --corpus ./examples/sample-corpus.jsonl
```

**What this tests:**
- Does chunking improve retrieval?
- Which chunking strategy works best?
- What's the optimal chunk size?

### Example 2: Hybrid Retrieval
```bash
embedeval ab-test \
  --name "Hybrid vs Pure Embedding" \
  --variants ollama:nomic-embed-text \
  --strategies baseline,hybrid-bm25 \
  --dataset ./examples/sample-queries.jsonl \
  --corpus ./examples/sample-corpus.jsonl
```

**What this tests:**
- Does BM25 + embeddings outperform pure embeddings?
- What's the optimal fusion weight?

### Example 3: Re-ranking Impact
```bash
embedeval ab-test \
  --name "Re-ranking Impact" \
  --variants openai:text-embedding-3-large \
  --strategies baseline,llm-reranked,mmr-diversity \
  --dataset ./examples/sample-queries.jsonl \
  --corpus ./examples/sample-corpus.jsonl
```

**What this tests:**
- Does LLM re-ranking improve top-k results?
- Does MMR improve result diversity?
- What's the latency cost of re-ranking?

### Example 4: Full Factorial Design
```bash
embedeval ab-test \
  --name "Complete Evaluation" \
  --variants ollama:nomic-embed-text,openai:text-embedding-3-small,openai:text-embedding-3-large \
  --strategies baseline,fixed-chunks,hybrid-bm25,llm-reranked,full-pipeline \
  --dataset ./examples/sample-queries.jsonl \
  --corpus ./examples/sample-corpus.jsonl
```

**What this tests:**
- Which model works best?
- Which strategy works best?
- What's the best model + strategy combination?
- Cost/quality trade-offs

## ðŸ“Š Research Questions You Can Now Answer

### Data Analysis Questions
1. **Does my data need chunking?**
   - Compare `baseline` vs `fixed-chunks` vs `semantic-chunks`

2. **What's the optimal chunk size?**
   - Create custom strategies with different sizes
   - Grid search: 128, 256, 512, 1024

3. **Does hybrid retrieval help?**
   - Compare `baseline` vs `hybrid-bm25`
   - Test different fusion weights

4. **Is re-ranking worth the cost?**
   - Compare `baseline` vs `llm-reranked`
   - Measure quality improvement vs latency cost

5. **Do I need diversity?**
   - Compare `baseline` vs `mmr-diversity`
   - Check for redundant results

### Model Selection Questions
1. **Which embedding model is best?**
   - Test multiple providers with same strategy

2. **Which strategy works best for my data?**
   - Test multiple strategies with same model

3. **What's the best combination?**
   - Full factorial: all models Ã— all strategies

## ðŸ”§ Extending the System

### Adding a New Chunking Strategy
```typescript
// src/strategies/chunking/my-chunker.ts
export class MyChunkingStage implements StrategyStage {
  name = 'my-chunking';
  type = 'chunking' as const;

  async execute(context: StrategyContext): Promise<StrategyContext> {
    // Your implementation
    context.chunks = myChunkingLogic(context.originalDocuments);
    return context;
  }
}

// Register in registry.ts
StrategyRegistry.register('chunking:my', MyChunkingStage);
```

### Adding a New Re-ranking Strategy
```typescript
// src/strategies/reranking/my-reranker.ts
export class MyRerankingStage implements StrategyStage {
  name = 'my-reranking';
  type = 'reranking' as const;

  async execute(context: StrategyContext): Promise<StrategyContext> {
    const docs = context.retrievedDocs || [];
    context.rerankedDocs = myRerankingLogic(docs);
    return context;
  }
}
```

### Creating Custom Strategy Pipeline
```typescript
const myStrategy = {
  name: 'my-strategy',
  description: 'Custom approach',
  stages: [
    { type: 'chunking', name: 'semantic', config: { maxSize: 512 } },
    { type: 'bm25', name: 'bm25', config: { k: 100 } },
    { type: 'embedding', name: 'embedding', config: {} },
    { type: 'retrieval', name: 'retrieval', config: { k: 100 } },
    { type: 'fusion', name: 'rrf', config: { k: 60, topK: 20 } },
    { type: 'reranking', name: 'llm', config: { topK: 10 } },
  ],
};
```

## ðŸŽ¯ Next Steps

### Immediate (High Priority)
1. âœ… Test the strategy system with real data
2. âœ… Add more predefined strategies
3. âœ… Create chunking experiment command
4. âœ… Add data analysis commands

### Short Term
1. Grid search for optimal parameters
2. Synthetic data generation
3. Time-series tracking
4. Advanced exports (Parquet, SQL)

### Long Term
1. AutoML for strategy selection
2. Learned fusion weights
3. Query-type routing
4. Ensemble methods

## ðŸ“ˆ Success Metrics

After this implementation, you can:
- âœ… Compare chunking strategies (fixed vs semantic vs sliding)
- âœ… Test hybrid retrieval (BM25 + embeddings)
- âœ… Evaluate re-ranking (LLM vs MMR)
- âœ… Run full factorial experiments (models Ã— strategies)
- âœ… Make data-driven architecture decisions

## ðŸŽ‰ Summary

**Before:** Compare embedding models only  
**After:** Research platform for retrieval architecture

**Key Innovation:** Composable strategy pipeline
- Chunking â†’ Embedding â†’ Retrieval â†’ Fusion â†’ Re-ranking
- Mix and match any combination
- Test any permutation of approaches

You now have a true **open-eval platform** for embedding research!

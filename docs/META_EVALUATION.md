# Meta-Evaluation: Evaluating the Evaluators

## Overview

This is a **self-referential evaluation** of EmbedEval's own evaluation capabilities. We used EmbedEval to evaluate different EmbedEval configurations, creating a meta-evaluation that demonstrates the system's ability to evaluate itself.

## What is Meta-Evaluation?

Meta-evaluation is the process of evaluating evaluation systems. In this case, we treated different evaluation configurations as the "models" being evaluated, and assessed them across multiple dimensions:

- **Accuracy**: How well does this eval config measure embedding quality?
- **Speed**: How fast does the evaluation run?
- **Cost**: What is the cost per query?
- **Reliability**: How consistent are the results?
- **Ease of Use**: How easy is it to set up and run?

## Results Summary

### üèÜ Winners by Category

| Category | Winner | Score |
|----------|--------|-------|
| **Best Accuracy** | Full Pipeline | 0.94 |
| **Best Speed** | Gemini Cloud | 0.92 (120ms) |
| **Best Cost** | Basic Baseline | 0.95 (Free) |
| **Best Reliability** | Multi-Provider | 0.94 |
| **Easiest to Use** | Gemini Cloud | 0.95 |
| **Overall Winner** | Hybrid BM25 | 0.80 |

### üìä Key Findings

1. **Hybrid strategies** (BM25 + embeddings) provide the best accuracy/cost tradeoff
2. **Cloud providers** (Gemini, OpenAI) significantly outperform local models in speed
3. **Local models** (Ollama, HuggingFace) are cost-effective but require more setup
4. **Advanced strategies** (MMR, reranking) improve accuracy but add complexity
5. **Multi-provider setups** offer best reliability but at higher cost
6. **Semantic chunking** outperforms fixed chunking for long documents

## Evaluation Configurations Tested

We tested 12 different evaluation configurations:

### Local/Free Options
- **Basic Baseline (Ollama)**: Simple, free, but slow
- **Chunking Strategy Test**: Better for long documents
- **HuggingFace Local**: Many models available
- **Minimal Config**: Fastest setup

### Cloud Options
- **Gemini Cloud**: Fast, easy, high quality
- **OpenAI Baseline**: Reliable, well-documented
- **Hybrid BM25 + Embeddings**: Best overall balance
- **MMR Diversity Reranking**: Diverse results
- **Full Pipeline**: Maximum accuracy
- **Semantic Chunking**: Smart boundaries
- **LLM Reranked**: Context-aware ranking
- **Multi-Provider Ensemble**: Highest reliability

## Methodology

### Dimensions Evaluated

Each configuration was scored 0-1 on five dimensions:

1. **Accuracy (35% weight)**: Based on NDCG, Recall, and MRR metrics
2. **Speed (20% weight)**: Average latency per query
3. **Cost (20% weight)**: Cost per query (normalized)
4. **Reliability (15% weight)**: Consistency across runs
5. **Ease of Use (10% weight)**: Setup complexity

### Overall Score Calculation

```
Overall = Accuracy√ó0.35 + Speed√ó0.20 + Cost√ó0.20 + Reliability√ó0.15 + Ease√ó0.10
```

## Viewing the Results

### Interactive Dashboard

Visit the GitHub Pages site to see interactive charts:
- Radar chart comparing top configurations
- Scatter plot showing cost vs accuracy trade-offs
- Detailed comparison table
- Configuration cards with pros/cons

### JSON Data

Raw results are available in `meta-evaluation-results.json` for programmatic access.

## Why Meta-Evaluation Matters

1. **Self-Validation**: Demonstrates EmbedEval can evaluate complex systems
2. **Configuration Guidance**: Helps users choose the right eval setup
3. **Best Practices**: Shows which strategies work best
4. **Trade-off Visualization**: Makes cost/quality trade-offs explicit

## GitHub Pages Deployment

The meta-evaluation results are deployed as a GitHub Pages site:

**URL**: `https://algiras.github.io/embedeval/`

### To Deploy/Update:

1. Push the `docs/` folder to your repository
2. Go to Settings ‚Üí Pages
3. Select "Deploy from a branch"
4. Choose "main" branch and "/docs" folder
5. Save and wait for deployment

## Files

- `docs/index.html` - Interactive GitHub Pages site
- `docs/meta-evaluation-results.json` - Raw evaluation data
- `docs/META_EVALUATION.md` - This documentation

## Next Steps

1. View the live dashboard at the GitHub Pages URL
2. Explore different configurations based on your needs
3. Use the findings to optimize your evaluation setup
4. Run your own meta-evaluation with real data

---

*This meta-evaluation was generated by EmbedEval evaluating itself. It's evaluations all the way down.* üê¢

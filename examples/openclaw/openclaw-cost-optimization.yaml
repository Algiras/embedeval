# OpenClaw Cost Optimization Example
# Compare expensive vs cheap embedding strategies as corpus grows

test:
  name: "Cost-Effective Knowledge Scaling"
  description: "Evaluate if cheaper models maintain quality as knowledge corpus scales"
  tags: ["openclaw", "cost-optimization", "scaling", "budget"]

variants:
  - id: premium-tier
    name: "Premium (OpenAI Large)"
    provider:
      type: openai
      model: text-embedding-3-large
    strategy: full-pipeline
    description: "Highest quality, most expensive"

  - id: standard-tier
    name: "Standard (OpenAI Small)"
    provider:
      type: openai
      model: text-embedding-3-small
    strategy: semantic-chunks
    description: "Good balance of quality and cost"

  - id: budget-tier
    name: "Budget (HuggingFace)"
    provider:
      type: huggingface
      model: sentence-transformers/all-MiniLM-L6-v2
    strategy: baseline
    description: "Cheapest option, test if quality is acceptable"

  - id: local-tier
    name: "Local (Ollama)"
    provider:
      type: ollama
      model: nomic-embed-text
    strategy: baseline
    description: "Free but slower, good for large batches"

dataset: ./data/openclaw-cost-queries.jsonl
corpus: ./data/openclaw-large-corpus.jsonl  # Large corpus for scaling test

metrics:
  - ndcg@10
  - recall@10
  - mrr@10

# Cost gates - ensure cheaper options are viable
gates:
  enabled: true
  failOnViolation: false  # Don't fail, just report
  cost:
    maxPerQuery: 0.001  # Flag if any variant exceeds $0.001/query
  metrics:
    ndcg@10:
      min: 0.70  # Minimum acceptable quality

output:
  json: ./results/openclaw-cost/metrics.json
  dashboard: ./results/openclaw-cost/dashboard.html
  csv: ./results/openclaw-cost/cost-analysis.csv

# Use this to decide:
# - Which tier to use for different corpus sizes
# - When to switch from cloud to local
# - Quality vs cost tradeoffs

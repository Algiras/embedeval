# MSMARCO Passage Ranking
# Large-scale passage retrieval benchmark
# Dataset: Microsoft MSMARCO (sample)

test:
  name: "MS MARCO Passage Ranking"
  description: "Large-scale passage retrieval from MSMARCO"
  tags: ["msmarco", "passage-ranking", "large-scale"]

variants:
  - id: msmarco-minilm
    name: "MiniLM"
    provider:
      type: huggingface
      model: sentence-transformers/all-MiniLM-L6-v2
    strategy: baseline
    description: "Lightweight baseline"

  - id: msmarco-e5
    name: "E5-large-v2"
    provider:
      type: huggingface
      model: intfloat/e5-large-v2
    strategy: hybrid-bm25
    description: "Strong baseline with hybrid"

  - id: msmarco-bge
    name: "BGE-large"
    provider:
      type: huggingface
      model: BAAI/bge-large-en-v1.5
    strategy: full-pipeline
    description: "Best accuracy with full pipeline"

  - id: msmarco-gte
    name: "GTE-large"
    provider:
      type: huggingface
      model: thenlper/gte-large
    strategy: semantic-chunks
    description: "Good balance with chunking"

dataset: ./datasets/msmarco/queries.jsonl
corpus: ./datasets/msmarco/corpus.jsonl

metrics:
  - ndcg@10
  - recall@50
  - recall@100
  - recall@1000
  - map@100
  - mrr@10

output:
  json: ./results/msmarco/metrics.json
  dashboard: ./results/msmarco/dashboard.html
  csv: ./results/msmarco/results.csv
  sideBySide: ./results/msmarco/comparison.html

cache:
  maxSizeGB: 15
  checkpointInterval: 10

# For large-scale evaluation, enable parallel processing
# Requires Redis to be running
parallel:
  enabled: true
  workers: 4
  batchSize: 32
